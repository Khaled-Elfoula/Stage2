P: 

To manipulate data effectively in Pandas, you need methods that allow you to transform, clean, and reshape your DataFrames. Here are the "heavy hitters" used in almost every AI project, categorized by what they do: 

 

1. The "Logic" Methods (Filtering & Modifying) 

These are used to create new features or isolate specific data points. 

.apply(): The "Swiss Army Knife." It applies a function to every row or column. 

Example: df['price_id'] = df['price'].apply(lambda x: "High" if x > 100 else "Low") 

.map(): Used for simple substitutions (e.g., turning "Male"/"Female" into 0/1). 

.drop(): Removes unwanted columns or rows. 

Example: df.drop(columns=['unnecessary_id'], inplace=True) 

.rename(): Changes column names so they are easier to type (e.g., {'Old Name': 'new_name'}). 

.sort_values(): Reorders your data by a specific column. 

 

2. The "Aggregator" Methods (Grouping) 

Crucial for turning raw data into meaningful summaries. 

.groupby(): Groups data based on a category (like "Coffee Type") so you can perform math on the groups. 

Example: df.groupby('Coffee Type')['Units Sold'].mean() 

.pivot_table(): Creates an Excel-style summary table. 

.value_counts(): Instantly tells you how many of each category exists (great for detecting Imbalanced Data). 

 

3. Handling Missing Values (The "Cleaning" Lab) 

In AI, missing values ($NaN$) will break your model. You have three main ways to handle them: 

Method 

What it do 

Best Use Case 

.isna().sum() 

Counts how many $NaN$ are in each column. 

Step 1: Use this to see how big the problem is. 

.dropna() 

Deletes any row (or column) that has a missing value. 

Use when you have millions of rows and losing a few doesn't matter. 

.fillna(value) 

Replaces the hole with a specific number (like the average). 

Imputation: Use when data is scarce. (e.g., df.fillna(df.mean())). 

.interpolate() 

Guesses the missing value based on the numbers before and after it. 

Best for Time-Series (like daily stock prices). 

 

4. Reshaping & Combining 

Sometimes your data is in the wrong "shape" for Scikit-Learn. 

.merge(): The Pandas version of a SQL JOIN. Combines two DataFrames based on a common key. 

.concat(): Stacks two DataFrames on top of each other or side-by-side. 

.melt(): Turns "wide" data into "long" data (useful for visualization). 

.reset_index(): After you group or filter, this turns your index back into a clean 0, 1, 2... column. 

 

Pro-Tip for AI: Vectorization 

Instead of using loops (for index, row in df.iterrows():), always try to use vectorized operations. 

Slow: Loop through and add 5 to every price. 

Fast: df['price'] = df['price'] + 5 

Pandas is optimized to do the math on the whole column at once! 

 

 

 

N: 

NumPy is the "engine" that powers Pandas and Scikit-Learn. While Pandas is for tables, NumPy is for Math and Arrays. In AI, we use it to turn images, text, and data into matrices (lists of numbers) that the computer can understand. 

Here are the most-used NumPy methods for data manipulation: 

 

1. The "Creation" Methods 

How you get data into a NumPy array ($ndarray$). 

np.array(): Converts a Python list into a high-speed NumPy array. 

np.zeros() / np.ones(): Creates arrays filled with 0s or 1s. Often used to initialize "Weights" in AI models. 

np.arange(start, stop, step): Like Python's range(), but returns an array. Great for creating a timeline for plots. 

np.linspace(0, 1, 100): Creates 100 evenly spaced numbers between 0 and 1. Perfect for testing "Thresholds" in classification. 

 

2. The "Shape-Shifter" Methods 

AI models are very picky about the "shape" of data. These are your most important tools: 

.reshape(rows, cols): Changes the structure without changing the data. 

Example: Turning a list of 784 pixels into a $28 \times 28$ image. 

.flatten(): Turns a multi-dimensional matrix back into a single long line. 

.T (Transpose): Flips a matrix (rows become columns). This is used constantly in the Backward Pass of Neural Networks. 

np.expand_dims(arr, axis=0): Adds a "fake" dimension. Useful when a model expects a "batch" of images but you only have one. 

 

3. The "Math & Aggregation" Methods 

These are vectorized (happening to all numbers at once), making them 100x faster than Python loops. 

np.mean() / np.std(): Calculates average and standard deviation (used for Standard Scaling). 

np.sum(axis=0): Sums columns ($axis=0$) or rows ($axis=1$). 

np.dot(a, b): Performs Matrix Multiplication. This is the fundamental math of every AI model. 

np.argmax(): Returns the index of the highest value. 

AI Use Case: If your model outputs probabilities $[0.1, 0.7, 0.2]$, argmax returns 1, telling you the AI chose the second category. 

 

4. Handling Missing & Outlier Values 

NumPy doesn't use "None"; it uses np.nan (Not a Number). 

Method 

What it do 

np.isnan(arr) 

Returns a "Mask" (True/False) showing where the holes are. 

np.where(condition, x, y) 

The "If-Else" of NumPy. If condition is true, use $x$, else use $y$. 

np.clip(arr, min, max) 

Handles Outliers by forcing values to stay within a range (e.g., don't let any value go above 100). 

np.unique() 

Like Pandas value_counts(), it finds all unique labels in your target $y$. 

 

5. Slicing and Masking 

This is how you "cut" data. 

arr[0:5, :]: Selects the first 5 rows and all columns. 

arr[arr > 0]: Boolean Masking. This pulls out only the positive numbers from an array. 

import numpy as np 

  

# 1. Create an array with a missing value (np.nan) 

data = np.array([10, 20, np.nan, 40, 50]) 

  

# 2. Calculate the average, ignoring the NaN 

# (If you used np.mean, the result would be NaN) 

avg = np.nanmean(data) 

  

# 3. If it's NaN, use avg; otherwise, keep the original value 

clean_data = np.where(np.isnan(data), avg, data) 

  

print(clean_data) 

# Output: [10. 20. 30. 40. 50.] 

 

 

 

 

 

 

 

 

 

 

 

 

 

S: 

Scikit-Learn (often imported as sklearn) is the industry-standard library for "Classical" Machine Learning. While NumPy and Pandas handle the data, Scikit-Learn provides the algorithms (the brains). 

Almost everything in Scikit-Learn follows the same "Law of Three Steps": Initialize → .fit() → .predict(). 

 

1. The "Pre-Game" (Preprocessing) 

Before the AI can learn, you must clean the data so the math works. 

train_test_split: The most used function. It hides data from the AI to prevent memorization. 

StandardScaler: Centers your data around 0. Essential for models that prioritize larger numbers. 

OneHotEncoder: Turns text (like "Apple", "Orange") into numbers (0, 1) that the model can process. 

SimpleImputer: The Scikit-Learn version of fillna. It fills in missing values automatically. 

 

2. The "Brains" (Most Used AI Models) 

Depending on your Target Label, you choose a different algorithm: 

Model 

Purpose 

AI Use Case 

LinearRegression 

Predicting a number. 

Estimating house prices. 

LogisticRegression 

Predicting a category. 

"Spam" vs "Not Spam". 

RandomForest 

A "Forest" of decision trees. 

High-accuracy classification (very robust). 

KMeans 

Unsupervised Learning. 

Grouping customers by shopping habits. 

SVM (Support Vector Machine) 

Finding a clear boundary. 

Image recognition or text classification. 

 

3. The "Training" Methods (The Workflow) 

This is the core syntax you will use 90% of the time: 

.fit(X, y): Starts the "Backward Pass." The model looks at features ($X$) and targets ($y$) to learn the patterns. 

.predict(X_new): The "Forward Pass." You give it new data, and it returns its best guess. 

.score(X, y): Gives you a quick accuracy reading. 

.predict_proba(X_new): Instead of just saying "Spam," it says "90% likely to be Spam." Essential for handling the Accuracy Paradox. 

 

4. The "Judge" (Evaluation Metrics) 

How we measure if the AI is actually smart or just lucky. 

accuracy_score: The percentage of correct guesses. 

confusion_matrix: A table showing exactly what the AI got confused by. 

classification_report: A "Report Card" that shows Precision, Recall, and F1-Score all at once. 

mean_squared_error: Measures those "red lines" for regression tasks. 

 

5. The "Professional" Tool: Pipeline 

In real AI jobs, we don't do steps one by one. We use a Pipeline to bundle the Scaler and the Model together into one single object. 

Python 

from sklearn.pipeline import make_pipeline 
 
# This bundle scales the data AND trains the model in one go! 
model = make_pipeline(StandardScaler(), RandomForestClassifier()) 
model.fit(X_train, y_train) 
 

 

 

 

PT: 

PyTorch is the "Formula 1" of AI libraries. While Scikit-Learn is for classical models, PyTorch is for Deep Learning (Neural Networks). It gives you total control over the math, which is why most AI researchers and companies (like Tesla and OpenAI) use it. 

In PyTorch, you don't just call a function; you often build a Class to define your "Brain". 

 

1. The Core Data Structure: Tensor 

In NumPy, you have "Arrays." In PyTorch, you have Tensors. 

The Difference: Tensors can live on your GPU, making them 50x–100x faster for AI math than NumPy. 

torch.tensor(): Converts data into a PyTorch Tensor. 

.to('cuda'): Moves your data from your CPU to your Graphics Card (NVIDIA GPU) for "lightning speed" training. 

 

2. The "Blueprint" (Building the Model) 

In PyTorch, you define your model by inheriting from nn.Module. 

nn.Linear(in, out): A standard layer of neurons. 

nn.ReLU(): An "Activation Function" that helps the AI learn complex, non-linear patterns (like curves instead of straight lines). 

nn.Sequential(): A quick way to stack layers on top of each other like LEGO bricks. 

Python 

import torch.nn as nn 
 
class MyBrain(nn.Module): 
    def __init__(self): 
        super().__init__() 
        self.layers = nn.Sequential( 
            nn.Linear(10, 5), # 10 inputs, 5 hidden neurons 
            nn.ReLU(),        # The "Learning" spark 
            nn.Linear(5, 1)   # 1 output (the prediction) 
        ) 
 
    def forward(self, x): 
        return self.layers(x) 
 

 

3. The "Engine" (Optimization) 

This is how the model performs the Backward Pass we discussed earlier. 

nn.MSELoss() / nn.CrossEntropyLoss(): These are the "Laws" we talked about to measure the Red Lines (Error). 

torch.optim.SGD / Adam: These are the Optimizers. They act like a GPS, telling the model exactly how to change its weights to reach the "bottom" of the error hill. 

loss.backward(): The most famous line in PyTorch. It automatically calculates the calculus (gradients) for the entire model. 

optimizer.step(): Actually updates the weights based on the gradients. 

 

4. The "Training Loop" (The Heartbeat) 

Unlike Scikit-Learn's simple .fit(), PyTorch requires you to write the loop yourself. This is where you see the Forward and Backward passes in action. 

Python 

for epoch in range(100): # Repeat 100 times 
    # 1. Forward Pass 
    predictions = model(X_train) 
    loss = criterion(predictions, y_train) 
     
    # 2. Backward Pass (The Learning) 
    optimizer.zero_grad() # Clear old math 
    loss.backward()       # Calculate new gradients 
    optimizer.step()      # Update the weights 
 

 

5. Utility Methods 

torch.save(model.state_dict(), 'model.pth'): Saves your trained AI to a file so you can use it later. 

DataLoader: Automatically splits your data into "Batches" (small bites) so the computer doesn't run out of memory. 

model.eval(): Turns off "Learning mode" so you can test the model on the Test Set without accidentally changing it. 

Summary: Scikit-Learn vs PyTorch 

Feature 

Scikit-Learn 

PyTorch 

Ease of Use 

Very Easy (.fit, .predict) 

Harder (Needs a loop) 

Best For 

Tabular data (Excel-like) 

Images, Video, Text, Audio 

Hardware 

Runs on CPU 

Runs on CPU & GPU 

Control 

"Black Box" (Automated) 

Full Control (Customizable) 

Would you like me to show you how to move a model to the GPU (cuda) so you can experience that 100x speed boost? 

 
